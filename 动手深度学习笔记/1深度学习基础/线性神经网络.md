**线性神经网络**

本章我们将介绍神经网络的整个训练过程， 包括：定义简单的神经网络架构、数据处理、指定损失函数和如何训练模型。 为了更容易学习，我们将从经典算法————线性神经网络开始，介绍神经网络的基础知识。 经典统计学习技术中的线性回归和softmax回归可以视为线性神经网络， 这些知识将为本书其他部分中更复杂的技术奠定基础。

# 线性回归

回归（regression）是能为**一个或多个自变量与因变量之间关系建模**的一类方法。 在自然科学和社会科学领域，回归经常用来**表示输入和输出之间的关系**。

## 线性回归的基本元素

线性回归（linear regression）可以追溯到19世纪初， 它在回归的各种标准工具中最简单而且最流行。 线性回归基于几个简单的假设： 首先，假设自变量 x 和因变量 y 之间的关系是线性的， 即 y 可以表示为 x 中元素的加权和，这里通常允许包含观测值的一些噪声； 其次，我们假设任何噪声都比较正常，如噪声遵循正态分布。

为了解释线性回归，我们举一个实际的例子： 我们希望根据房屋的面积（平方英尺）和房龄（年）来估算房屋价格（美元）。 为了开发一个能预测房价的模型，我们需要收集一个真实的数据集。 这个数据集包括了房屋的销售价格、面积和房龄。 在机器学习的术语中，该数据集称为训练数据集（training data set） 或训练集（training set）。 每行数据（比如一次房屋交易相对应的数据）称为样本（sample）， 也可以称为数据点（data point）或数据样本（data instance）。 我们把试图预测的目标（比如预测房屋价格）称为标签（label）或目标（target）。 预测所依据的自变量（面积和房龄）称为特征（feature）或协变量（covariate）。

### 线性模型

> 案例

线性假设是指目标（房屋价格）可以表示为特征（面积和房龄）的加权和，如下面的式子：

$\mathrm{price} = w_{\mathrm{area}} \cdot \mathrm{area} + w_{\mathrm{age}} \cdot \mathrm{age} + b$

- w 称为权重（weight），权重决定了每个特征对我们预测值的影响。  
- b 称为偏置（bias）、偏移量（offset）或截距（intercept）。 偏置是指当所有特征都取值为0时，预测值应该为多少。 即使现实中不会有任何房子的面积是0或房龄正好是0年，我们仍然需要偏置项。 如果没有偏置项，我们模型的表达能力将受到限制。
- 该公式是输入特征的一个 *仿射变换*（affine transformation）。 仿射变换的特点是通过加权和对特征进行*线性变换*（linear transformation）， 并通过偏置项来进行*平移*（translation）。

> 线性模型定义

给定一个数据集，我们的目标是寻找模型的权重 w 和偏置 b ， 使得根据模型做出的预测大体符合数据里的真实价格。 输出的预测值由输入特征通过线性模型的仿射变换决定，仿射变换由所选权重和偏置确定。

而在机器学习领域，我们通常使用的是高维数据集，建模时采用线性代数表示法会比较方便。 当我们的输入包含 d 个特征时，我们将预测结果 $\hat{y}$ （通常使用“尖角”符号表示 y 的估计值）表示为：

$\hat{y} = w_1  x_1 + ... + w_d  x_d + b$

将所有特征放到向量 x 中， 并将所有权重放到向量 w中， 我们可以用点积形式来简洁地表达模型：

$\hat{y} = \mathbf{w}^\top \mathbf{x} + b$

**向量 x 对应于单个数据样本的特征**。 用符号表示的**矩阵 X** 可以很方便地引用我们整个数据集的 n 个样本。 其中， X 的每一行是一个样本，每一列是一种特征。

${\hat{\mathbf{y}}} = \mathbf{X} \mathbf{w} + b$

这个过程中的求和将使用广播机制。

给定训练数据特征 X 和对应的已知标签 y ， 线性回归的目标是找到一组权重向量 w 和偏置 b ： 当给定从 X 的同分布中取样的新样本特征时， 这组权重向量和偏置能够使得新样本预测标签的误差尽可能小。

虽然我们相信给定 x 预测 y 的最佳模型会是线性的， 但我们很难找到一个有 n 个样本的真实数据集完全贴合模型。无论我们使用什么手段来观察特征 X 和标签 y ， 都可能会出现少量的观测误差。 因此，即使确信特征与标签的潜在关系是线性的， 我们也会加入一个噪声项来考虑观测误差带来的影响。

在开始寻找最好的模型参数（model parameters） w 和 b 之前， 我们还需要两个东西： （1）一种模型质量的度量方式； （2）一种能够更新模型以提高模型预测质量的方法。

### 损失函数

在我们开始考虑如何用模型拟合（fit）数据之前，我们需要确定一个拟合程度的度量。 损失函数（loss function）能够量化目标的实际值与预测值之间的差距。 通常我们会选择非负数作为损失，且数值越小表示损失越小，完美预测时的损失为0。 回归问题中最常用的损失函数是平方误差函数。 当样本 i 的预测值为 y^(i) ，其相应的真实标签为 y(i) 时， 平方误差可以定义为以下公式：

$l^{(i)}(\mathbf{w}, b) = \frac{1}{2} \left(\hat{y}^{(i)} - y^{(i)}\right)^2$

常数 1/2 不会带来本质的差别，但这样在形式上稍微简单一些 （因为当我们对损失函数求导后常数系数为1）。 由于训练数据集并不受我们控制，所以经验误差只是关于模型参数的函数。

![image-20220130143249249](image-20220130143249249.png)

由于平方误差函数中的二次方项， 估计值 $\hat{y}^{(i)}$ 和观测值$ y^{(i)}$ 之间较大的差异将导致更大的损失。 为了度量模型在整个数据集上的质量，我们需计算在训练集 n 个样本上的损失均值（也等价于求和）。

$L(\mathbf{w}, b) =\frac{1}{n}\sum_{i=1}^n l^{(i)}(\mathbf{w}, b) =\frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right)^2$

在训练模型时，我们希望寻找一组参数（ w∗,b∗ ）， 这组参数能最小化在所有训练样本上的总损失。如下式：

$\mathbf{w}^*, b^* = \operatorname*{argmin}_{\mathbf{w}, b}\  L(\mathbf{w}, b)$

### 解析解

线性回归刚好是一个很简单的优化问题。 与我们将在本书中所讲到的其他大部分模型不同，线性回归的解可以用一个公式简单地表达出来， 这类解叫作解析解（analytical solution）。

首先，我们将偏置 b 合并到参数 w 中，合并方法是在包含所有参数的矩阵中附加一列。 我们的预测问题是最小化$\|\mathbf{y} - \mathbf{X}\mathbf{w}\|^2$

这在损失平面上只有一个临界点(二次函数)，这个临界点对应于整个区域的损失极小点。 将损失关于w的导数设为0，得到解析解：

$\mathbf{w}^* = (\mathbf X^\top \mathbf X)^{-1}\mathbf X^\top \mathbf{y}$

像线性回归这样的简单问题存在解析解，但并不是所有的问题都存在解析解。 **解析解可以进行很好的数学分析，但解析解对问题的限制很严格，导致它无法广泛应用在深度学习里。**

### 随机梯度下降

**即使在我们无法得到解析解的情况下，我们仍然可以有效地训练模型。 在许多任务上，那些难以优化的模型效果要更好。 因此，弄清楚如何训练这些难以优化的模型是非常重要的。**

**本书中我们用到一种名为梯度下降（gradient descent）的方法， 这种方法几乎可以优化所有深度学习模型。 它通过不断地在损失函数递减的方向上更新参数来降低误差。**

梯度下降最简单的用法是计算损失函数（数据集中所有样本的损失均值） 关于模型参数的导数（在这里也可以称为梯度）。 但实际中的执行可能会非常慢：因为在每一次更新参数之前，我们必须遍历整个数据集。 因此，我们通常会在每次需要计算更新的时候随机抽取一小批样本， 这种变体叫做小批量随机梯度下降（minibatch stochastic gradient descent）。

在每次迭代中，我们首先随机抽样一个小批量 B ， 它是由固定数量的训练样本组成的。 然后，我们计算小批量的平均损失关于模型参数的导数（也可以称为梯度）。 最后，我们将梯度乘以一个预先确定的正数 η ，并从当前参数的值中减掉。

我们用下面的数学公式来表示这一更新过程（ ∂ 表示偏导数）：

$(\mathbf{w},b) \leftarrow (\mathbf{w},b) - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{(\mathbf{w},b)} l^{(i)}(\mathbf{w},b)$

总结一下，算法的步骤如下： （1）初始化模型参数的值，如随机初始化； （2）从数据集中随机抽取小批量样本且在负梯度的方向上更新参数，并不断迭代这一步骤。 对于平方损失和仿射变换，我们可以明确地写成如下形式:

$\begin{split}\begin{aligned} \mathbf{w} &\leftarrow \mathbf{w} -   \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_{\mathbf{w}} l^{(i)}(\mathbf{w}, b) = \mathbf{w} - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \mathbf{x}^{(i)} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right),\\ b &\leftarrow b -  \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \partial_b l^{(i)}(\mathbf{w}, b)  = b - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} \left(\mathbf{w}^\top \mathbf{x}^{(i)} + b - y^{(i)}\right). \end{aligned}\end{split}$

|B| 表示每个小批量中的样本数，这也称为批量大小（batch size）。  η 表示学习率（learning rate）。 批量大小和学习率的值通常是手动预先指定，而不是通过模型训练得到的。 这些可以调整但不在训练过程中更新的参数称为超参数（hyperparameter）。 调参（hyperparameter tuning）是选择超参数的过程。 超参数通常是我们根据训练迭代结果来调整的， 而训练迭代结果是在独立的验证数据集（validation dataset）上评估得到的。

在训练了预先确定的若干迭代次数后（或者直到满足某些其他停止条件后）， 我们记录下模型参数的估计值，表示为$\hat{\mathbf{w}}, \hat{b}$。 但是，即使我们的函数确实是线性的且无噪声，这些估计值也不会使损失函数真正地达到最小值。 因为算法会使得损失向最小值缓慢收敛，但却不能在有限的步数内非常精确地达到最小值。

线性回归恰好是一个在整个域中只有一个最小值的学习问题。 但是对于像深度神经网络这样复杂的模型来说，损失平面上通常包含多个最小值。 深度学习实践者很少会去花费大力气寻找这样一组参数，使得在训练集上的损失达到最小。 事实上，更难做到的是找到一组参数，这组参数能够在我们从未见过的数据上实现较低的损失， 这一挑战被称为泛化（generalization）。

## 矢量化加速

在训练我们的模型时，我们经常希望能够同时处理整个小批量的样本。 为了实现这一点，需要我们对计算进行矢量化， 从而利用线性代数库，而不是在Python中编写开销高昂的for循环。

 矢量化代码通常会带来数量级的加速。 另外，我们将更多的数学运算放到库中，而无须自己编写那么多的计算，从而减少了出错的可能性。

## 正态分布与平方损失

接下来，我们通过对噪声分布的假设来解读平方损失目标函数。

正态分布和线性回归之间的关系很密切。 正态分布（normal distribution），也称为*高斯分布*（Gaussian distribution）， 最早由德国数学家高斯（Gauss）应用于天文学研究。 简单的说，若随机变量xx具有均值μμ和方差σ2σ2（标准差σσ），其正态分布概率密度函数如下：

$p(x) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (x - \mu)^2\right)$

下面我们定义一个Python函数来计算正态分布。

```
def normal(x, mu, sigma):
    p = 1 / math.sqrt(2 * math.pi * sigma**2)
    return p * np.exp(-0.5 / sigma**2 * (x - mu)**2)
```

我们现在可视化正态分布。

```
# 再次使用numpy进行可视化
x = np.arange(-7, 7, 0.01)

# 均值和标准差对
params = [(0, 1), (0, 2), (3, 1)]
d2l.plot(x, [normal(x, mu, sigma) for mu, sigma in params], xlabel='x',
         ylabel='p(x)', figsize=(4.5, 2.5),
         legend=[f'mean {mu}, std {sigma}' for mu, sigma in params])
```

![image-20220130150148323](image-20220130150148323.png)

就像我们所看到的，改变均值会产生沿x轴的偏移，增加方差将会分散分布、降低其峰值。

**均方误差损失函数（简称均方损失）可以用于线性回归的一个原因是： 我们假设了观测中包含噪声，其中噪声服从正态分布。** 噪声正态分布如下式:

​	$y = \mathbf{w}^\top \mathbf{x} + b + \epsilon$      其中$\epsilon \sim \mathcal{N}(0, \sigma^2)$

因此，我们现在可以写出通过给定的 x 观测到特定 y 的似然（likelihood）:

$P(y \mid \mathbf{x}) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp\left(-\frac{1}{2 \sigma^2} (y - \mathbf{w}^\top \mathbf{x} - b)^2\right)$

(在统计学上，基于某些模型的参数（粗略地说，我们可以认为参数决定了模型），观测到某数据的概率称为概率；而已经观测到某数据，模型的参数取特定值的概率称为似然。举个例子来说，我们有一些秤，这些秤不怎么准，每次称东西都会有一些误差，而且每次称的误差可能不一样（比方说，使用了一些不稳定的电子元件）。假设这些误差是由这些秤的零件的参数决定的。那么，给定一个秤，并且有出厂报告，上面载明了零件的参数。拿这杆秤去称个1kg的东西，可能称出各种重量，比如0.98kg、0.99kg、1.02kg等等，称出每种重量的概率，称为概率。另一方面，有一个秤的出厂报告我们丢了，现在我们要通过拿这个秤去反复称1kg的东西，根据测出的重量的统计数据，反推这个秤的参数可能是哪些值（参数取每个值的概率），这就称为似然。)

现在，根据极大似然估计法，参数 w 和 b 的最优值是使整个数据集的似然最大的值：

$P(\mathbf y \mid \mathbf X) = \prod_{i=1}^{n} p(y^{(i)}|\mathbf{x}^{(i)})$

根据极大似然估计法选择的估计量称为极大似然估计量。 虽然使许多指数函数的乘积最大化看起来很困难， 但是我们可以在不改变目标的前提下，通过最大化似然对数来简化。 由于历史原因，优化通常是说最小化而不是最大化。 我们可以改为最小化负对数似然$-\log P(\mathbf y \mid \mathbf X)$ 由此可以得到数学公式：

$-\log P(\mathbf y \mid \mathbf X) = \sum_{i=1}^n \frac{1}{2} \log(2 \pi \sigma^2) + \frac{1}{2 \sigma^2} \left(y^{(i)} - \mathbf{w}^\top \mathbf{x}^{(i)} - b\right)^2$

现在我们只需要假设 σ 是某个固定常数就可以忽略第一项， 因为第一项不依赖于 w 和 b 。 现在第二项除了常数$\frac{1}{\sigma^2}$外，其余部分和前面介绍的均方误差是一样的。 幸运的是，上面式子的解并不依赖于 σ 。 因此，在高斯噪声的假设下，最小化均方误差等价于对线性模型的极大似然估计。



## 从线性回归到深度网络

到目前为止，我们只谈论了线性模型。 尽管神经网络涵盖了更多更为丰富的模型，我们依然可以用描述神经网络的方式来描述线性模型， 从而把线性模型看作一个神经网络。 首先，我们用“层”符号来重写这个模型。

### 神经网络图

将线性回归模型描述为一个神经网络。 需要注意的是，该图只显示连接模式，即只显示每个输入如何连接到输出，隐去了权重和偏置的值。

![image-20220130150804644](image-20220130150804644.png)

输入为$x_1, \ldots, x_d$，因此输入层中的输入数（或称为特征维度，feature dimensionality）为 d 。 网络的输出为 o1 ，因此输出层中的输出数是1。

需要注意的是，输入值都是已经给定的，并且只有一个*计算*神经元。 由于模型重点在发生计算的地方，所以通常我们在计算层数时不考虑输入层。

**我们可以将线性回归模型视为仅由单个人工神经元组成的神经网络，或称为单层神经网络。**

对于线性回归，每个输入都与每个输出（在本例中只有一个输出）相连， 我们将这种变换 称为*全连接层*（fully-connected layer）或称为*稠密层*（dense layer）。



# 线性回归的从零实现

```
%matplotlib inline
import random
import torch
from d2l import torch as d2l
```

## 数据集生成

为了简单起见，我们将根据带有噪声的线性模型构造一个人造数据集。 我们的任务是使用这个有限样本的数据集来恢复这个模型的参数。 我们将使用低维数据，这样可以很容易地将其可视化。 在下面的代码中，我们生成一个包含1000个样本的数据集， 每个样本包含从标准正态分布中采样的2个特征。 我们的合成数据集是一个矩阵 X。

我们使用线性模型参数$\mathbf{w} = [2, -3.4]^\top,b=4.2$和噪声项 ϵ 生成数据集及其标签：

$\mathbf{y}= \mathbf{X} \mathbf{w} + b + \mathbf\epsilon$

你可以将 ϵ 视为模型预测和标签时的潜在观测误差。 在这里我们认为标准假设成立，即 ϵ 服从均值为0的正态分布。 为了简化问题，我们将标准差设为0.01。 下面的代码生成合成数据集。

```python

def synthetic_data(w, b, num_of_examples):
    """
        定义样本生成函数
        生成 y=wx+b+噪声 的样本，y=wx+b是原来的模型，噪声是服从正态分布偏离模型。
        输入模型和样本数，自动生成噪声为正态分布的对应模型的样本
    """
    X = torch.normal(0, 1, (num_of_examples, len(w)))
    # 生成均值是0，标准差是1，的随机数
    # 输出为一个 张量，具体维度为（num_of_examples行，len(w)列）
    # 代表了um_of_examples行有um_of_examples这么多个样本
    # len(w)列说明每个样本点的维度数时按照w参数的维度来的，即模型的特征（属性）数量。
    y = torch.matmul(X, w)+b
    # 矩阵乘法：torch.matmul , X样本矩阵先线性运算得到正确的结果,y的形状应当是[numberofexamples,1]。
    y += torch.normal(0, 0.01, y.shape)
    # 对运算的结果加上噪声偏差值得到生成的正式样本。生成的张量的形状保持原来输入不变。
    return X, y.reshape((-1, 1))
    # X是正确的标签矩阵，y是样本，reshape(-1,1)说明输出成一列数据


true_w = torch.tensor([2, -3.4])  # 输入模型的权重参数
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)
# 生成1000个样本x和1000个标签y

```

注意，`features`中的每一行都包含一个二维数据样本(w1,w2)， `labels`中的每一行都包含一维标签值（一个标量）。

```python
print('features:', features[0],'\nlabel:', labels[0])

features: tensor([ 0.6631, -0.7805])
label: tensor([8.1842])
```

## 读取数据集

回想一下，训练模型时要对数据集进行遍历，每次抽取一小批量样本，并使用它们来更新我们的模型。 由于这个过程是训练机器学习算法的基础，所以有必要定义一个函数， 该函数能打乱数据集中的样本并以小批量方式获取数据。

在下面的代码中，我们定义一个`data_iter`函数， 该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为`batch_size`的小批量。 每个小批量包含一组特征和标签。

```python
def data_iter(batch_size, features, labels):
    """
        定义batch生成函数:输入是batch大小，输入X（特征矩阵）和标签值y
    """
    num_of_examples = len(features)
    # 这里的features是个[1000,2]的张量，使用len()函数返回的是行数
    indices = list(range(num_of_examples))
    # 获取样本数量之后，用range函数生成一个从0开始的递增序列保存在列表中，一共有样本那么多个
    random.shuffle(indices)
    # 把这个列表乱序，形成随机
    for i in range(0, num_of_examples, batch_size):
        batch_indice = torch.tensor(
            indices[i:min(i+batch_size, num_of_examples)])
        # 指定batch中的内容，从乱序的indices列表中连续读取，读到最后一块时，若不够长度则自成一块。
        yield features[batch_size], labels[batch_size]
        # 每次返回batchsize大小的X与Y

```

通常，我们利用GPU并行运算的优势，处理合理大小的“小批量”。 每个样本都可以并行地进行模型计算，且每个样本损失函数的梯度也可以被并行计算。 GPU可以在处理几百个样本时，所花费的时间不比处理一个样本时多太多。

## 初始化模型参数

在我们开始用小批量随机梯度下降优化我们的模型参数之前， 我们需要先有一些参数。 在下面的代码中，我们通过从均值为0、标准差为0.01的正态分布中采样随机数来初始化权重， 并将偏置初始化为0。

```
w = torch.normal(0, 0.01, size=(2,1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
```

在初始化参数之后，我们的任务是更新这些参数，直到这些参数足够拟合我们的数据。 每次更新都需要计算损失函数关于模型参数的梯度。 有了这个梯度，我们就可以向减小损失的方向更新每个参数。 因为手动计算梯度很枯燥而且容易出错，所以没有人会手动计算梯度。

## 定义模型

接下来，我们必须定义模型，将模型的输入和参数同模型的输出关联起来。 回想一下，要计算线性模型的输出， 我们只需计算输入特征 X 和模型权重 w 的矩阵-向量乘法后加上偏置 b 。 注意，上面的 Xw 是一个向量，而 b 是一个标量。广播机制： 当我们用一个向量加一个标量时，标量会被加到向量的每个分量上。

```
def linreg(X, w, b):  #@save
    """线性回归模型"""
    return torch.matmul(X, w) + b
```

## 定义损失函数

因为需要计算损失函数的梯度，所以我们应该先定义损失函数。 

这里使用平方损失函数。

```
def squared_loss(y_hat, y):  #@save
    """均方损失"""
    return (y_hat - y.reshape(y_hat.shape)) ** 2 / 2
```

在实现中，我们需要将真实值`y`的形状转换为和预测值`y_hat`的形状相同。

## 定义优化算法

线性回归有解析解。 尽管线性回归有解析解，但本书中的其他模型却没有。 这里我们介绍小批量随机梯度下降。

在每一步中，使用从数据集中随机抽取的一个小批量，然后根据参数计算损失的梯度。 接下来，朝着减少损失的方向更新我们的参数。 下面的函数实现小批量随机梯度下降更新。 该函数接受模型参数集合、学习速率和批量大小作为输入。每 一步更新的大小由学习速率`lr`决定。 因为我们计算的损失是一个批量样本的总和，所以我们用批量大小（`batch_size`） 来规范化步长，这样步长大小就不会取决于我们对批量大小的选择。

```

def sgd(params, lr, batch_size):
    # 定义随机梯度下降函数，输入为参数集（可以理解为梯度信息），学习速率，batch大小
    with torch.no_grad():
        # 这里已经在更新计算梯度了，所以参数/梯度向量避免再次计算梯度的梯度
        for param in params:
            param -= lr*param.grad/batch_size
            param.grad.zero_()
```

## 训练

现在我们已经准备好了模型训练所有需要的要素，可以实现主要的训练过程部分了。 理解这段代码至关重要，因为从事深度学习后， 你会一遍又一遍地看到几乎相同的训练过程。 在每次迭代中，我们读取一小批量训练样本，并通过我们的模型来获得一组预测。 计算完损失后，我们开始反向传播，存储每个参数的梯度。 最后，我们调用优化算法`sgd`来更新模型参数。

括一下，我们将执行以下循环：

- 初始化参数
- 重复以下训练，直到完成
  - 计算梯度$\mathbf{g} \leftarrow \partial_{(\mathbf{w},b)} \frac{1}{|\mathcal{B}|} \sum_{i \in \mathcal{B}} l(\mathbf{x}^{(i)}, y^{(i)}, \mathbf{w}, b)$
  - 更新参数$(\mathbf{w}, b) \leftarrow (\mathbf{w}, b) - \eta \mathbf{g}$

在每个*迭代周期*（epoch）中，我们使用`data_iter`函数遍历整个数据集， 并将训练数据集中所有样本都使用一次（假设样本数能够被批量大小整除）。 这里的迭代周期个数`num_epochs`和学习率`lr`都是超参数，分别设为3和0.03。 设置超参数很棘手，需要通过反复试验进行调整。

```
lr = 0.03
num_of_epches = 3
net = lineregression  # 搭建网络模型
loss = squard_loss
batch_size = 100

# 训练过程
for epoch in range(num_of_epches):
    for X, y in data_iter(batch_size, features, labels):
        l = squard_loss(net(X, w, b), y)  # l的形状是[batch_size,1]
        l.sum().backward()   # 累加成为标量后计算梯度，这里的梯度存储起来，等待para.grad调用
        sgd([w, b], lr, batch_size)  # 使用sgd更新参数，内部调用上面算出的梯度信息
    with torch.no_grad(): #等一个epoch执行完，输出loss信息
        train_l = loss(net(features, w, b), labels)
        print(f'epoch{epoch+1},loss{float(train_l.mean()):f}')
        # 字符串前加f说明串内支持{}的表达式
        
epoch 1, loss 0.040067
epoch 2, loss 0.000148
epoch 3, loss 0.000049
```

因为我们使用的是自己合成的数据集，所以我们知道真正的参数是什么。 因此，我们可以通过比较真实参数和通过训练学到的参数来评估训练的成功程度。 事实上，真实参数和通过训练学到的参数确实非常接近。

```
print(f'w的估计误差: {true_w - w.reshape(true_w.shape)}')
print(f'b的估计误差: {true_b - b}')

w的估计误差: tensor([ 0.0005, -0.0003], grad_fn=<SubBackward0>)
b的估计误差: tensor([-0.0001], grad_fn=<RsubBackward1>)
```

注意，我们不应该想当然地认为我们能够完美地求解参数。 在机器学习中，我们通常不太关心恢复真正的参数，而更关心如何高度准确预测参数。 幸运的是，即使是在复杂的优化问题上，随机梯度下降通常也能找到非常好的解。 其中一个原因是，在深度网络中存在许多参数组合能够实现高度精确的预测。

## 完整代码

```

from __future__ import print_function
from turtle import back
import d2l
import torch
import random
from ast import increment_lineno


def synthetic_data(w, b, num_of_examples):
    """
        定义样本生成函数
        生成 y=wx+b+噪声 的样本，y=wx+b是原来的模型，噪声是服从正态分布偏离模型。
        输入模型和样本数，自动生成噪声为正态分布的对应模型的样本
    """
    X = torch.normal(0, 1, (num_of_examples, len(w)))
    # 生成均值是0，标准差是1，的随机数
    # 输出为一个 张量，具体维度为（num_of_examples行，len(w)列）
    # 代表了um_of_examples行有um_of_examples这么多个样本
    # len(w)列说明每个样本点的维度数时按照w参数的维度来的，即模型的特征（属性）数量。
    y = torch.matmul(X, w)+b
    # 矩阵乘法：torch.matmul , X样本矩阵先线性运算得到正确的结果。
    y += torch.normal(0, 0.01, y.shape)
    # 对运算的结果加上噪声偏差值得到生成的正式样本。生成的张量的形状保持原来输入不变。
    return X, y.reshape((-1, 1))
    # X是正确的标签矩阵，y是样本，reshape(-1,1)说明输出成一列数据


true_w = torch.tensor([2, -3.4])  # 输入模型的权重参数
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)
# 生成1000个样本和标签,X是（1000,2），y是（1000，1）


def data_iter(batch_size, features, labels):
    """
        定义batch生成函数:输入是batch大小，输入X（特征矩阵）和标签值y
    """
    num_of_examples = len(features)
    # 这里的features是个[1000,2]的张量，使用len()函数返回的是行数
    indices = list(range(num_of_examples))
    # 获取样本数量之后，用range函数生成一个从0开始的递增序列保存在列表中，一共有样本那么多个
    random.shuffle(indices)
    # 把这个列表乱序，形成随机
    for i in range(0, num_of_examples, batch_size):
        batch_indice = torch.tensor(
            indices[i:min(i+batch_size, num_of_examples)])
        # 指定batch中的内容，从乱序的indices列表中连续读取，读到最后一块时，若不够长度则自成一块。
        yield features[batch_size], labels[batch_size]
        # 每次返回batchsize大小的X与Y


w = torch.normal(0, 0.01, size=(2, 1), requires_grad=True)
b = torch.zeros(1, requires_grad=True)
# 初始化模型的参数，b直接置0，w从正态分布中直接取样


def lineregression(X, w, b):
    # 定义线性回归模型/网络模型
    return torch.matmul(X, w)+b


def squard_loss(y_hat, y):
    # 定义平方损失函数
    return (y_hat-y.reshape(y_hat.shape))**2/2


def sgd(params, lr, batch_size):
    # 定义随机梯度下降函数，输入为参数集（可以理解为梯度信息），学习速率，batch大小
    with torch.no_grad():
        # 这里已经在更新计算梯度了，所以参数/梯度向量避免再次计算梯度的梯度
        for param in params:
            param -= lr*param.grad/batch_size
            param.grad.zero_()


lr = 0.03
num_of_epches = 3
net = lineregression  # 搭建网络模型
loss = squard_loss
batch_size = 10

# 训练过程
for epoch in range(num_of_epches):
    for X, y in data_iter(batch_size, features, labels):
        l = squard_loss(net(X, w, b), y)  # l的形状是[batch_size,1]
        l.sum().backward()   # 累加成为标量后计算梯度，这里的梯度存储起来，等待para.grad调用
        sgd([w, b], lr, batch_size)  # 使用sgd更新参数，内部调用上面算出的梯度信息
    with torch.no_grad():  # 等一个epoch执行完，输出loss信息
        train_l = loss(net(features, w, b), labels)
        print(f'epoch{epoch+1},loss{float(train_l.mean()):f}')
        # 字符串前加f说明串内支持{}的表达式

print(f'w的估计误差:{true_w-w.reshape(true_w.shape)}')
print(f'b的估计误差:{true_b-b}')

```

# 线性回归的简洁实现

在过去的几年里，出于对深度学习强烈的兴趣， 许多公司、学者和业余爱好者开发了各种成熟的开源框架。 这些框架可以自动化基于梯度的学习算法中重复性的工作。

我们只运用了： （1）通过张量来进行数据存储和线性代数； （2）通过自动微分来计算梯度。 实际上，由于数据迭代器、损失函数、优化器和神经网络层很常用， 现代深度学习库也为我们实现了这些组件。

## 生成数据集

```
import numpy as np
import torch
from torch.utils import data
from d2l import torch as d2l

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = d2l.synthetic_data(true_w, true_b, 1000)
```

## 读取数据集

**调用框架中现有的API来读取数据。 我们将`features`和`labels`作为API的参数传递，并通过数据迭代器指定`batch_size`。 此外，布尔值`is_train`表示是否希望数据迭代器对象在每个迭代周期内打乱数据。**

```
def load_array(data_arrays, batch_size, is_train=True):  #@save
    """构造一个PyTorch数据迭代器"""
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle=is_train)

batch_size = 10
data_iter = load_array((features, labels), batch_size)
```

## 定义模型

对于标准深度学习模型，**我们可以使用框架的预定义好的层。这使我们只需关注使用哪些层来构造模型，而不必关注层的实现细节**。 我们首先定义一个模型变量`net`，它是一个`Sequential`类的实例。 `Sequential`类将多个层串联在一起。 当给定输入数据时，`Sequential`实例将数据传入到第一层， 然后将第一层的输出作为第二层的输入，以此类推。 在下面的例子中，我们的模型只包含一个层，因此实际上不需要`Sequential`。 **但是由于以后几乎所有的模型都是多层的，在这里使用`Sequential`会让你熟悉“标准的流水线”。**

单层网络架构， 这一单层被称为*全连接层*（fully-connected layer）， 因为它的每一个输入都通过矩阵-向量乘法得到它的每个输出。

在PyTorch中，全连接层在`Linear`类中定义。 值得注意的是，我们将两个参数传递到`nn.Linear`中。 第一个指定输入特征形状，即2，第二个指定输出特征形状，输出特征形状为单个标量，因此为1。

## 初始化模型参数

在使用`net`之前，我们需要初始化模型参数。 如在线性回归模型中的权重和偏置。 深度学习框架通常有预定义的方法来初始化参数。 在这里，我们指定每个权重参数应该从均值为0、标准差为0.01的正态分布中随机采样， 偏置参数将初始化为零。

正如我们在构造`nn.Linear`时指定输入和输出尺寸一样， 现在我们能直接访问参数以设定它们的初始值。 我们**通过`net[0]`选择网络中的第一个图层**， 然后使用`weight.data`和`bias.data`方法访问参数。 我们还可以使用替换方法`normal_`和`fill_`来重写参数值。

```
net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)
```

## 定义损失函数

计算均方误差使用的是`MSELoss`类，也称为平方L2范数。 默认情况下，它返回所有样本损失的平均值。

```
loss = nn.MSELoss()
```

## 定义优化算法

小批量随机梯度下降算法是一种优化神经网络的标准工具， PyTorch在optim模块中实现了该算法的许多变种。 当我们实例化一个SGD实例时，我们要指定优化的参数 （可通过net.parameters()从我们的模型中获得）以及优化算法所需的超参数字典。 小批量随机梯度下降只需要设置lr值，这里设置为0.03。

```
trainer = torch.optim.SGD(net.parameters(), lr=0.03)
```

## 训练

通过深度学习框架的高级API来实现我们的模型只需要相对较少的代码。 我们不必单独分配参数、不必定义我们的损失函数，也不必手动实现小批量随机梯度下降。 当我们需要更复杂的模型时，高级API的优势将大大增加。 当我们有了所有的基本组件，训练过程代码与我们从零开始实现时所做的非常相似。

回顾一下：在每个迭代周期里，我们将完整遍历一次数据集（`train_data`）， 不停地从中获取一个小批量的输入和相应的标签。 对于每一个小批量，我们会进行以下步骤:

- 通过调用`net(X)`生成预测并计算损失`l`（前向传播）。
- 通过进行反向传播来计算梯度。
- 通过调用优化器来更新模型参数。

为了更好的衡量训练效果，我们计算每个迭代周期后的损失，并打印它来监控训练过程。

```
num_epochs = 3
for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X) ,y)
        trainer.zero_grad()
        l.backward()
        trainer.step()
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')
```

## 整体代码

```
from random import shuffle
import re
import torch
from torch.utils import data
from torch import nn


def synthetic_data(w, b, num_of_examples):
    """
        定义样本生成函数
        生成 y=wx+b+噪声 的样本，y=wx+b是原来的模型，噪声是服从正态分布偏离模型。
        输入模型和样本数，自动生成噪声为正态分布的对应模型的样本
    """
    X = torch.normal(0, 1, (num_of_examples, len(w)))
    # 生成均值是0，标准差是1，的随机数
    # 输出为一个 张量，具体维度为（num_of_examples行，len(w)列）
    # 代表了um_of_examples行有um_of_examples这么多个样本
    # len(w)列说明每个样本点的维度数时按照w参数的维度来的，即模型的特征（属性）数量。
    y = torch.matmul(X, w)+b
    # 矩阵乘法：torch.matmul , X样本矩阵先线性运算得到正确的结果。
    y += torch.normal(0, 0.01, y.shape)
    # 对运算的结果加上噪声偏差值得到生成的正式样本。生成的张量的形状保持原来输入不变。
    return X, y.reshape((-1, 1))
    # X是正确的标签矩阵，y是样本，reshape(-1,1)说明输出成一列数据


true_w = torch.tensor([2, -3.4])  # 输入模型的权重参数
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000)
# 生成1000个样本和标签,X是（1000,2），y是（1000，1）


def load_array(data_arrays, batch_size, is_train):
    # 构造一个Pytorch的数据迭代器
    dataset = data.TensorDataset(*data_arrays)  # 构造数据集
    return data.DataLoader(dataset, batch_size, shuffle=is_train)


batch_size = 10
data_iter = load_array((features, labels), batch_size, is_train=True)

net = nn.Sequential(nn.Linear(2, 1))

net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)

loss = nn.MSELoss()

trainer = torch.optim.SGD(net.parameters(), lr=0.03)

num_of_epoches = 3
for epoch in range(num_of_epoches):
    for X, y in data_iter:
        l = loss(net(X), y)
        trainer.zero_grad()
        l.backward()  # 此处已在loss函数中完成累加了，不需要.sum().backward()
        trainer.step()
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')


w = net[0].weight.data
print('w的估计误差：', true_w - w.reshape(true_w.shape))
b = net[0].bias.data
print('b的估计误差：', true_b - b)

```

# softmax回归

回归可以用于预测*多少*的问题。

事实上，我们也对*分类*问题感兴趣：不是问“多少”，而是问“哪一个”。

通常，机器学习实践者用*分类*这个词来描述两个有微妙差别的问题： 1. 我们只对样本的“硬性”类别感兴趣，即属于哪个类别； 2. 我们希望得到“软性”类别，即得到属于每个类别的概率。 这两者的界限往往很模糊。其中的一个原因是：即使我们只关心硬类别，我们仍然使用软类别的模型。

## 分类问题

我们从一个图像分类问题开始。 假设每次输入是一个2×22×2的灰度图像。 我们可以用一个标量表示每个像素值，每个图像对应四个特征x1,x2,x3,x4x1,x2,x3,x4。 此外，假设每个图像属于类别“猫”，“鸡”和“狗”中的一个。

接下来，我们要选择如何表示标签。 我们有两个明显的选择：最直接的想法是选择$y \in \{1, 2, 3\}$,其中整数分别代表狗猫鸡。 这是在计算机上存储此类信息的有效方法。 如果类别间有一些自然顺序， 比如说我们试图预测$\{\text{婴儿}, \text{儿童}, \text{青少年}, \text{青年人}, \text{中年人}, \text{老年人}\}$那么将这个问题转变为回归问题，并且保留这种格式是有意义的。

但是一般的分类问题并不与类别之间的自然顺序有关。 幸运的是，统计学家很早以前就发明了一种表示分类数据的简单方法：*独热编码*（one-hot encoding）。 独热编码是一个向量，它的分量和类别一样多。 类别对应的分量设置为1，其他所有分量设置为0。 

在我们的例子中，标签yy将是一个三维向量， $y \in \{(1, 0, 0), (0, 1, 0), (0, 0, 1)\}$



## 网络架构

为了估计所有可能类别的条件概率，我们需要一个有多个输出的模型，每个类别对应一个输出。

为了解决线性模型的分类问题，我们需要和输出一样多的*仿射函数*（affine function）。 每个输出对应于它自己的仿射函数。 在我们的例子中，由于我们有4个特征和3个可能的输出类别， 我们将需要12个标量来表示权重（带下标的 w ）， 3个标量来表示偏置（带下标的 b ）。

下面我们为每个输入计算三个未规范化的预测（logit）： o1 、 o2 和 o3 。

$\begin{split}\begin{aligned} o_1 &= x_1 w_{11} + x_2 w_{12} + x_3 w_{13} + x_4 w_{14} + b_1,\\ o_2 &= x_1 w_{21} + x_2 w_{22} + x_3 w_{23} + x_4 w_{24} + b_2,\\ o_3 &= x_1 w_{31} + x_2 w_{32} + x_3 w_{33} + x_4 w_{34} + b_3. \end{aligned}\end{split}$

 与线性回归一样，softmax回归也是一个单层神经网络。 由于计算每个输出 o1 、 o2 和 o3 取决于 所有输入 x1 、 x2 、 x3 和 x4 ， 所以softmax回归的输出层也是全连接层。

![image-20220201142929108](image-20220201142929108.png)

为了更简洁地表达模型，我们仍然使用线性代数符号。 通过向量形式表达为 o=Wx+b ， 这是一种更适合数学和编写代码的形式。 由此，我们已经将所有权重放到一个 3×4 矩阵中。 对于给定数据样本的特征 x ， 我们的输出是由权重与输入特征进行矩阵-向量乘法再加上偏置 b 得到的。



## 全连接层的参数开销

在深度学习中，全连接层无处不在。 然而，顾名思义，全连接层是“完全”连接的，可能有很多可学习的参数。 具体来说，对于任何具有 d 个输入和 q 个输出的全连接层， 参数开销为 O(dq) ，这个数字在实践中可能高得令人望而却步。 幸运的是，将 d 个输入转换为 q 个输出的成本可以减少到 O(dq/n) ， 其中超参数 n 可以由我们灵活指定，以在实际应用中平衡参数节约和模型有效性

## softmax运算

现在我们将优化参数以最大化观测数据的概率。 为了得到预测结果，我们将设置一个阈值，如选择具有最大概率的标签。

我们希望模型的输出$\hat{y}_j$可以视为属于类j的概率， 然后选择具有最大输出值的类别$\operatorname*{argmax}_j y_j$作为我们的预测。

然而我们能否将未规范化的预测 o 直接视作我们感兴趣的输出呢？ 答案是否定的。 因为将线性层的输出直接视为概率时存在一些问题： 一方面，我们没有限制这些输出数字的总和为1。 另一方面，根据输入的不同，它们可以为负值。 

要将输出视为概率，我们必须保证在任何数据上的输出都是非负的且总和为1。 此外，我们需要一个训练目标，来鼓励模型精准地估计概率。 在分类器输出0.5的所有样本中，我们希望这些样本有一半实际上属于预测的类。 这个属性叫做校准（calibration）。

社会科学家邓肯·卢斯于1959年在选择模型（choice model）的理论基础上 发明的softmax函数正是这样做的： softmax函数将未规范化的预测变换为非负并且总和为1，同时要求模型保持可导。 我们首先对每个未规范化的预测求幂，这样可以确保输出非负。 为了确保最终输出的总和为1，我们再对每个求幂后的结果除以它们的总和。如下式：

$\hat{\mathbf{y}} = \mathrm{softmax}(\mathbf{o})\quad \text{其中}\quad \hat{y}_j = \frac{\exp(o_j)}{\sum_k \exp(o_k)}$

softmax运算不会改变未规范化的预测 o 之间的顺序，只会确定分配给每个类别的概率。 因此，在预测过程中，我们仍然可以用下式来选择最有可能的类别。

$\operatorname*{argmax}_j \hat y_j = \operatorname*{argmax}_j o_j$

尽管softmax是一个非线性函数，但softmax回归的输出仍然由输入特征的仿射变换决定。 因此，softmax回归是一个*线性模型*（linear model）。

## 小批量样本的矢量化

为了提高计算效率并且充分利用GPU，我们通常会针对小批量数据执行矢量计算。 假设我们读取了一个批量的样本 X ， 其中特征维度（输入数量）为 d ，批量大小为 n 。 此外，假设我们在输出中有 q 个类别。 

 那么小批量特征为X(nxd),权重为W(dxq),偏置为b(1xq),softmax回归的矢量计算表达式为：

$\begin{split}\begin{aligned} \mathbf{O} &= \mathbf{X} \mathbf{W} + \mathbf{b}, \\ \hat{\mathbf{Y}} & = \mathrm{softmax}(\mathbf{O}). \end{aligned}\end{split}$

相对于一次处理一个样本， 小批量样本的矢量化加快了 X和W 的矩阵-向量乘法。 由于 X 中的每一行代表一个数据样本， 那么softmax运算可以按行（rowwise）执行： 对于 O 的每一行，我们先对所有项进行幂运算，然后通过求和对它们进行标准化。 在 (3.4.5)中，  XW+b 的求和会使用广播， 小批量的未规范化预测 O 和输出概率 Y^  都是形状为 n×q 的矩阵。

## 损失函数

需要一个损失函数来度量预测的效果。 我们将使用最大似然估计，这与在线性回归中的方法相同。

### 对数似然

softmax函数给出了一个向量$\hat{y}$ , 我们可以将其视为“对给定任意输入 x 的每个类的条件概率”。

假设整个数据集 {X,Y} 具有 n 个样本， 其中索引 i 的样本由特征向量 $x^{(i)}$和独热(one-hot)标签向量 $y^{(i)}$ 组成。 我们可以将估计值与实际值进行比较：

$P(\mathbf{Y} \mid \mathbf{X}) = \prod_{i=1}^n P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}).$

根据最大似然估计，我们最大化 P(Y∣X) ，相当于最小化负对数似然：

$-\log P(\mathbf{Y} \mid \mathbf{X}) = \sum_{i=1}^n -\log P(\mathbf{y}^{(i)} \mid \mathbf{x}^{(i)}) = \sum_{i=1}^n l(\mathbf{y}^{(i)}, \hat{\mathbf{y}}^{(i)})$

其中，对于任何标签 y 和模型预测 $\hat{y}$，损失函数为：

$l(\mathbf{y}, \hat{\mathbf{y}}) = - \sum_{j=1}^q y_j \log \hat{y}_j$

上式损失函数 通常被称为*交叉熵损失*（cross-entropy loss）。

由于 y 是一个长度为 q 的独热编码向量， 所以除了一个项以外的所有项 j 都消失了。

由于所有 $\hat{y}_j$ 都是预测的概率，所以它们的对数永远不会大于 0 。 因此，如果正确地预测实际标签，即如果实际标签 P(y∣x)=1 ， 则损失函数不能进一步最小化。 注意，这往往是不可能的。 例如，数据集中可能存在标签噪声（比如某些样本可能被误标）， 或输入特征没有足够的信息来完美地对每一个样本分类。



### softmax及其导数

由于softmax和相关的损失函数很常见， 因此我们需要更好地理解它的计算方式。

利用softmax的定义，我们得到：

$\begin{split}\begin{aligned}
l(\mathbf{y}, \hat{\mathbf{y}}) &=  - \sum_{j=1}^q y_j \log \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} \\
&= \sum_{j=1}^q y_j \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j\\
&= \log \sum_{k=1}^q \exp(o_k) - \sum_{j=1}^q y_j o_j.
\end{aligned}\end{split}$

考虑相对于任何未规范化的预测 oj 的导数，我们得到：

$\partial_{o_j} l(\mathbf{y}, \hat{\mathbf{y}}) = \frac{\exp(o_j)}{\sum_{k=1}^q \exp(o_k)} - y_j = \mathrm{softmax}(\mathbf{o})_j - y_j$

换句话说，导数是我们softmax模型分配的概率与实际发生的情况（由独热标签向量表示）之间的差异。 从这个意义上讲，这与我们在回归中看到的非常相似， 其中梯度是观测值 y 和估计值 y^ 之间的差异。 这不是巧合，在任何指数族分布模型中 ， 对数似然的梯度正是由此得出的。 这使梯度计算在实践中变得容易很多。

### 交叉熵损失

现在让我们考虑整个结果分布的情况，即观察到的不仅仅是一个结果。 对于标签 y ，我们可以使用与以前相同的表示形式。 唯一的区别是，我们现在用一个概率向量表示，如 (0.1,0.2,0.7) ， 而不是仅包含二元项的向量 (0,0,1) 。

定义损失 l ， 它是所有标签分布的预期损失值。 此损失称为交叉熵损失（cross-entropy loss），它是分类问题最常用的损失之一。

## 信息论

*信息论*（information theory）涉及编码、解码、发送以及尽可能简洁地处理信息或数据。

### 熵

信息论的核心思想是量化数据中的信息内容。 在信息论中，该数值被称为分布 P 的熵（entropy）。可以通过以下方程得到：

$H[P] = \sum_j - P(j) \log P(j)$

(数值对应的概率与偏离数值发生的概率做乘积)

**熵， 是当分配的概率真正匹配数据生成过程时的*预期惊异*（expected surprisal）。**

信息论的基本定理之一指出，为了对从分布 p 中随机抽取的数据进行编码， 我们至少需要 H[P] “纳特（nat）”对其进行编码。 “纳特”相当于比特（bit），但是对数底为 e 而不是2。



### 惊异

压缩与预测有什么关系呢？ 想象一下，我们有一个要压缩的数据流。 如果我们很容易预测下一个数据，那么这个数据很容易压缩。 为什么呢？ 举一个极端的例子，假如数据流中的每个数据完全相同，这会是一个非常无聊的数据流。 由于它们总是相同的，所以很容易被预测。 所以，为了传递数据流的内容，我们不必传输任何信息。 因此，当数据易于预测，也就易于压缩。

但是，如果我们不能完全预测每一个事件，那么我们有时可能会感到“惊异”。 克劳德·香农决定用$\log \frac{1}{P(j)} = -\log P(j)$来量化惊异（surprisal）。 在观察一个事件 j ，并赋予它（主观）概率 P(j) 。 当我们赋予一个事件较低的概率时，我们的惊异会更大。

熵， 是当分配的概率真正匹配数据生成过程时的预期惊异（expected surprisal）。



### 从惊异理解交叉熵

如果把熵H(P)H(P)想象为“知道真实概率的人所经历的惊异程度”，那么什么是交叉熵？ 交叉熵*从*PP*到*QQ，记为H(P,Q)H(P,Q)。 你可以把交叉熵想象为“主观概率为QQ的观察者在看到根据概率PP生成的数据时的预期惊异”。 当P=QP=Q时，交叉熵达到最低。 在这种情况下，从PP到QQ的交叉熵是H(P,P)=H(P)H(P,P)=H(P)。

简而言之，我们可以从两方面来考虑交叉熵分类目标： （i）最大化观测数据的似然；（ii）最小化传达标签所需的惊异。



## 模型预测和评估

在训练softmax回归模型后，给出任何样本特征，我们可以预测每个输出类别的概率。 通常我们使用预测概率最高的类别作为输出类别。 如果预测与实际类别（标签）一致，则预测是正确的。 在接下来的实验中，我们将使用精度（accuracy）来评估模型的性能。 精度等于正确预测数与预测总数之间的比率。



## 小结

- softmax运算获取一个向量并将其映射为概率。
- softmax回归适用于分类问题，它使用了softmax运算中输出类别的概率分布。
- 交叉熵是一个衡量两个概率分布之间差异的很好的度量，它测量给定模型编码数据所需的比特数。



# 图像分类数据集

MNIST数据集 [LeCun et al., 1998] 是图像分类中广泛使用的数据集之一，但作为基准数据集过于简单。 我们将使用类似但更复杂的Fashion-MNIST数据集。

```
%matplotlib inline
import torch
import torchvision
from torch.utils import data
from torchvision import transforms
from d2l import torch as d2l

d2l.use_svg_display() #使用svg显示图片，这样清晰度高
```

## 读取数据集

我们可以通过框架中的内置函数将Fashion-MNIST数据集下载并读取到内存中。

```python
# 通过torchvison.transforms.ToTensor将图片转换成tensor格式
trans = torchvision.transforms.ToTensor()
# 定义数据集
mnist_train_set = torchvision.datasets.FashionMNIST(
    root="C:/Users/libangmao/Desktop/test_dataset", train=True, transform=trans, download=True)
# 定义测试集
mnist_test_set = torchvision.datasets.FashionMNIST(
    root="C:/Users/libangmao/Desktop/test_dataset", train=False, transform=trans, download=True)

```

Fashion-MNIST由10个类别的图像组成， 每个类别由训练数据集（train dataset）中的6000张图像 和测试数据集（test dataset）中的1000张图像组成。 因此，训练集和测试集分别包含60000和10000张图像。 测试数据集不会用于训练，只用于评估模型性能。

```
len(mnist_train), len(mnist_test)

(60000, 10000)
```

每个输入图像的高度和宽度均为28像素。 数据集由灰度图像组成，其通道数为1。 为了简洁起见，本书将高度h像素、宽度w像素图像的形状记为h×w或（h,w）。

```python
mnist_train[0][0].shape
# [0][0]，前一个0表示第一个样本，后一个0表示图片的标号，指的是第一个样本中的第一个图片
#mnist_train[1]是(img,label)  所以用[0][0]
#如果用[0][1]则表示标签
torch.Size([1, 28, 28])
#是黑白图片，所以第一个维度通道数是1
```

Fashion-MNIST中包含的10个类别，分别为t-shirt（T恤）、trouser（裤子）、pullover（套衫）、dress（连衣裙）、coat（外套）、sandal（凉鞋）、shirt（衬衫）、sneaker（运动鞋）、bag（包）和ankle boot（短靴）。 以下函数用于在数字标签索引及其文本名称之间进行转换。

```
def get_fashion_mnist_labels(labels):  #@save
    """返回Fashion-MNIST数据集的文本标签"""
    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',
                   'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
    return [text_labels[int(i)] for i in labels]
```

我们现在可以创建一个函数来可视化这些样本。

```
def show_images(imgs, num_rows, num_cols, titles=None, scale=1.5):  #@save
    """绘制图像列表"""
    figsize = (num_cols * scale, num_rows * scale)
    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize=figsize)
    axes = axes.flatten()
    for i, (ax, img) in enumerate(zip(axes, imgs)):
        if torch.is_tensor(img):
            # 图片张量
            ax.imshow(img.numpy())
        else:
            # PIL图片
            ax.imshow(img)
        ax.axes.get_xaxis().set_visible(False)
        ax.axes.get_yaxis().set_visible(False)
        if titles:
            ax.set_title(titles[i])
    return axes
```

以下是训练数据集中前几个样本的图像及其相应的标签。

```
X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))
show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y));
```

![image-20220201164849962](image-20220201164849962.png)



## 读取mini-batch

为了使我们在读取训练集和测试集时更容易，我们使用内置的数据迭代器，而不是从零开始创建。 回顾一下，在每次迭代中，数据加载器每次都会读取一小批量数据，大小为batch_size。 通过内置数据迭代器，我们可以随机打乱了所有样本，从而无偏见地读取mini-batch。

```
batch_size = 256

def get_dataloader_workers():  #@save
    """使用4个进程来读取数据"""
    return 4

train_iter = data.DataLoader(mnist_train, batch_size, shuffle=True,
                             num_workers=get_dataloader_workers())
```

我们看一下读取训练数据所需的时间。

```
timer = d2l.Timer()
for X, y in train_iter:
    continue
f'{timer.stop():.2f} sec'

'1.69 sec'
```

## 整合所有组件

现在我们定义`load_data_fashion_mnist`函数，用于获取和读取Fashion-MNIST数据集。 这个函数返回训练集和验证集的数据迭代器。 此外，这个函数还接受一个可选参数`resize`，用来将图像大小调整为另一种形状。

```
def load_data_fashion_mnist(batch_size, resize=None):  #@save
    """下载Fashion-MNIST数据集，然后将其加载到内存中"""
    trans = [transforms.ToTensor()]
    if resize:
        trans.insert(0, transforms.Resize(resize))
    trans = transforms.Compose(trans)
    mnist_train = torchvision.datasets.FashionMNIST(
        root="../data", train=True, transform=trans, download=True)
    mnist_test = torchvision.datasets.FashionMNIST(
        root="../data", train=False, transform=trans, download=True)
    return (data.DataLoader(mnist_train, batch_size, shuffle=True,
                            num_workers=get_dataloader_workers()),
            data.DataLoader(mnist_test, batch_size, shuffle=False,
                            num_workers=get_dataloader_workers()))
```

下面，我们通过指定`resize`参数来测试`load_data_fashion_mnist`函数的图像大小调整功能。

```
train_iter, test_iter = load_data_fashion_mnist(32, resize=64)
for X, y in train_iter:
    print(X.shape, X.dtype, y.shape, y.dtype)
    break
    
torch.Size([32, 1, 64, 64]) torch.float32 torch.Size([32]) torch.int64
```

## 小结

- Fashion-MNIST是一个服装分类数据集，由10个类别的图像组成。我们将在后续章节中使用此数据集来评估各种分类算法。
- 我们将高度hh像素，宽度ww像素图像的形状记为h×wh×w或（hh,ww）。
- 数据迭代器是获得更高性能的关键组件。依靠实现良好的数据迭代器，利用高性能计算来避免减慢训练过程。

# softmax回归的从零实现

就像我们从零开始实现线性回归一样， 我们认为softmax回归也是重要的基础，因此你应该知道实现softmax回归的细节。 本节我们将使用刚刚在 3.5节中引入的Fashion-MNIST数据集， 并设置数据迭代器的批量大小为256。

```
import torch
from IPython import display
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

## 初始化模型参数

和之前线性回归的例子一样，这里的每个样本都将用固定长度的向量表示。 原始数据集中的每个样本都是 28×28 的图像。 在本节中，我们将展平每个图像，把它们看作长度为784的向量。 在后面的章节中，我们将讨论能够利用图像空间结构的特征， 但现在我们暂时只把每个像素位置看作一个特征。

回想一下，在softmax回归中，我们的输出与类别一样多。 因为我们的数据集有10个类别，所以网络输出维度为10。 因此，权重将构成一个 784×10 的矩阵， 偏置将构成一个 1×10 的行向量。 与线性回归一样，我们将使用正态分布初始化我们的权重W，偏置初始化为0。

```
num_inputs = 784
num_outputs = 10

W = torch.normal(0, 0.01, size=(num_inputs, num_outputs), requires_grad=True)
b = torch.zeros(num_outputs, requires_grad=True)
```

## 定义softmax操作

给定一个矩阵`X`，我们可以对所有元素求和（默认情况下）。 也可以只求同一个轴上的元素，即同一列（轴0）或同一行（轴1）。 如果`X`是一个形状为`(2, 3)`的张量，我们对列进行求和， 则结果将是一个具有形状`(3,)`的向量。 当调用`sum`运算符时，我们可以指定保持在原始张量的轴数，而不折叠求和的维度。 这将产生一个具有形状`(1, 3)`的二维张量。

```
X = torch.tensor([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
X.sum(0, keepdim=True), X.sum(1, keepdim=True)
# 按行求和
(tensor([[5., 7., 9.]]),
 tensor([[ 6.],
         [15.]]))
```

回想一下，实现softmax由三个步骤组成：

1. 对每个项求幂（使用`exp`）；
2. 对每一行求和（小批量中每个样本是一行），得到每个样本的规范化常数；
3. 将每一行除以其规范化常数，确保结果的和为1。

在查看代码之前，我们回顾一下这个表达式：

$\mathrm{softmax}(\mathbf{X})_{ij} = \frac{\exp(\mathbf{X}_{ij})}{\sum_k \exp(\mathbf{X}_{ik})}$

分母或规范化常数，有时也称为配分函数（其对数称为对数-配分函数）。 该名称来自统计物理学中一个模拟粒子群分布的方程。

```
def softmax(X):
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdim=True)
    return X_exp / partition  # 这里应用了广播机制
```

正如你所看到的，对于任何随机输入，我们将每个元素变成一个非负数。 此外，依据概率原理，每行总和为1。

```
X = torch.normal(0, 1, (2, 5))
X_prob = softmax(X)
X_prob, X_prob.sum(1)

(tensor([[0.0553, 0.1871, 0.5327, 0.0711, 0.1538],
         [0.1365, 0.2855, 0.2733, 0.0312, 0.2735]]),
 tensor([1.0000, 1.0000]))
```

注意，虽然这在数学上看起来是正确的，但我们在代码实现中有点草率。 矩阵中的非常大或非常小的元素可能造成数值上溢或下溢，但我们没有采取措施来防止这点。

## 定义模型

定义softmax操作后，我们可以实现softmax回归模型。 下面的代码定义了输入如何通过网络映射到输出。 注意，将数据传递到模型之前，我们使用`reshape`函数将每张原始图像展平为向量。

```
def net(X):
    return softmax(torch.matmul(X.reshape((-1, W.shape[0])), W) + b)
```

## 定义损失函数

接下来，我们实现 [3.4节](https://zh.d2l.ai/chapter_linear-networks/softmax-regression.html#sec-softmax)中引入的交叉熵损失函数。 这可能是深度学习中最常见的损失函数，因为目前分类问题的数量远远超过回归问题的数量。

回顾一下，交叉熵采用真实标签的预测概率的负对数似然。 这里我们不使用Python的for循环迭代预测（这往往是低效的）， 而是通过一个运算符选择所有元素。 下面，我们创建一个数据样本`y_hat`，其中包含2个样本在3个类别的预测概率， 以及它们对应的标签`y`。 有了`y`，我们知道在第一个样本中，第一类是正确的预测； 而在第二个样本中，第三类是正确的预测。 然后使用`y`作为`y_hat`中概率的索引， 我们选择第一个样本中第一个类的概率和第二个样本中第三个类的概率。

```
y = torch.tensor([0, 2])
y_hat = torch.tensor([[0.1, 0.3, 0.6], [0.3, 0.2, 0.5]])
y_hat[[0, 1], y]

tensor([0.1000, 0.5000])
```

现在我们只需一行代码就可以实现交叉熵损失函数。

```
def cross_entropy(y_hat, y):
    return - torch.log(y_hat[range(len(y_hat)), y])

cross_entropy(y_hat, y)

tensor([2.3026, 0.6931])
```



## 分类精度

给定预测概率分布y_hat，当我们必须输出硬预测（hard prediction）时， 我们通常选择预测概率最高的类。 许多应用都要求我们做出选择。如Gmail必须将电子邮件分类为“Primary（主要邮件）”、 “Social（社交邮件）”、“Updates（更新邮件）”或“Forums（论坛邮件）”。 Gmail做分类时可能在内部估计概率，但最终它必须在类中选择一个。

当预测与标签分类`y`一致时，即是正确的。 分类精度即正确预测数量与总预测数量之比。 虽然直接优化精度可能很困难（因为精度的计算不可导）， 但精度通常是我们最关心的性能衡量标准，我们在训练分类器时几乎总会关注它。

为了计算精度，我们执行以下操作。 首先，如果`y_hat`是矩阵，那么假定第二个维度存储每个类的预测分数。 我们使用`argmax`获得每行中最大元素的索引来获得预测类别。 然后我们将预测类别与真实`y`元素进行比较。 由于等式运算符“`==`”对数据类型很敏感， 因此我们将`y_hat`的数据类型转换为与`y`的数据类型一致。 结果是一个包含0（错）和1（对）的张量。 最后，我们求和会得到正确预测的数量。

```
def accuracy(y_hat, y):  #@save
    """计算预测正确的数量"""
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    return float(cmp.type(y.dtype).sum())
```

我们将继续使用之前定义的变量`y_hat`和`y`分别作为预测的概率分布和标签。 可以看到，第一个样本的预测类别是2（该行的最大元素为0.6，索引为2），这与实际标签0不一致。 第二个样本的预测类别是2（该行的最大元素为0.5，索引为2），这与实际标签2一致。 因此，这两个样本的分类精度率为0.5。

```
accuracy(y_hat, y) / len(y)
```

同样，对于任意数据迭代器`data_iter`可访问的数据集， 我们可以评估在任意模型`net`的精度。

```
def evaluate_accuracy(net, data_iter):  #@save
    """计算在指定数据集上模型的精度"""
    if isinstance(net, torch.nn.Module):
        net.eval()  # 将模型设置为评估模式
    metric = Accumulator(2)  # 正确预测数、预测总数
    with torch.no_grad():
        for X, y in data_iter:
            metric.add(accuracy(net(X), y), y.numel())
    return metric[0] / metric[1]
```

这里定义一个实用程序类`Accumulator`，用于对多个变量进行累加。 在上面的`evaluate_accuracy`函数中， 我们在`Accumulator`实例中创建了2个变量， 分别用于存储正确预测的数量和预测的总数量。 当我们遍历数据集时，两者都将随着时间的推移而累加。

```
class Accumulator:  #@save
    """在n个变量上累加"""
    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]
```

由于我们使用随机权重初始化`net`模型， 因此该模型的精度应接近于随机猜测。 例如在有10个类别情况下的精度为0.1。

```
evaluate_accuracy(net, test_iter)
```

## 训练

在这里，我们重构训练过程的实现以使其可重复使用。 首先，我们定义一个函数来训练一个迭代周期。 请注意，`updater`是更新模型参数的常用函数，它接受批量大小作为参数。 它可以是`d2l.sgd`函数，也可以是框架的内置优化函数。

```
def train_epoch_ch3(net, train_iter, loss, updater):  #@save
    """训练模型一个迭代周期（定义见第3章）"""
    # 将模型设置为训练模式
    if isinstance(net, torch.nn.Module):
        net.train()
    # 训练损失总和、训练准确度总和、样本数
    metric = Accumulator(3)
    for X, y in train_iter:
        # 计算梯度并更新参数
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            # 使用PyTorch内置的优化器和损失函数
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            # 使用定制的优化器和损失函数
            l.sum().backward()
            updater(X.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # 返回训练损失和训练精度
    return metric[0] / metric[2], metric[1] / metric[2]
```

在展示训练函数的实现之前，我们定义一个在动画中绘制数据的实用程序类`Animator`， 它能够简化本书其余部分的代码。

```
class Animator:  #@save
    """在动画中绘制数据"""
    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
                 ylim=None, xscale='linear', yscale='linear',
                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,
                 figsize=(3.5, 2.5)):
        # 增量地绘制多条线
        if legend is None:
            legend = []
        d2l.use_svg_display()
        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)
        if nrows * ncols == 1:
            self.axes = [self.axes, ]
        # 使用lambda函数捕获参数
        self.config_axes = lambda: d2l.set_axes(
            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
        self.X, self.Y, self.fmts = None, None, fmts

    def add(self, x, y):
        # 向图表中添加多个数据点
        if not hasattr(y, "__len__"):
            y = [y]
        n = len(y)
        if not hasattr(x, "__len__"):
            x = [x] * n
        if not self.X:
            self.X = [[] for _ in range(n)]
        if not self.Y:
            self.Y = [[] for _ in range(n)]
        for i, (a, b) in enumerate(zip(x, y)):
            if a is not None and b is not None:
                self.X[i].append(a)
                self.Y[i].append(b)
        self.axes[0].cla()
        for x, y, fmt in zip(self.X, self.Y, self.fmts):
            self.axes[0].plot(x, y, fmt)
        self.config_axes()
        display.display(self.fig)
        display.clear_output(wait=True)
```

接下来我们实现一个训练函数， 它会在`train_iter`访问到的训练数据集上训练一个模型`net`。 该训练函数将会运行多个迭代周期（由`num_epochs`指定）。 在每个迭代周期结束时，利用`test_iter`访问到的测试数据集对模型进行评估。 我们将利用`Animator`类来可视化训练进度。

```
def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  #@save
    """训练模型（定义见第3章）"""
    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                        legend=['train loss', 'train acc', 'test acc'])
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        animator.add(epoch + 1, train_metrics + (test_acc,))
    train_loss, train_acc = train_metrics
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc
```

作为一个从零开始的实现，我们使用 [3.2节](https://zh.d2l.ai/chapter_linear-networks/linear-regression-scratch.html#sec-linear-scratch)中定义的 小批量随机梯度下降来优化模型的损失函数，设置学习率为0.1。

```
lr = 0.1

def updater(batch_size):
    return d2l.sgd([W, b], lr, batch_size)
```

现在，我们训练模型10个迭代周期。 请注意，迭代周期（`num_epochs`）和学习率（`lr`）都是可调节的超参数。 通过更改它们的值，我们可以提高模型的分类精度。

```
num_epochs = 10
train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)
```

## 预测

现在训练已经完成，我们的模型已经准备好对图像进行分类预测。 给定一系列图像，我们将比较它们的实际标签（文本输出的第一行）和模型预测（文本输出的第二行）。

```
def predict_ch3(net, test_iter, n=6):  #@save
    """预测标签（定义见第3章）"""
    for X, y in test_iter:
        break
    trues = d2l.get_fashion_mnist_labels(y)
    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))
    titles = [true +'\n' + pred for true, pred in zip(trues, preds)]
    d2l.show_images(
        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])

predict_ch3(net, test_iter)
```

## 小结

- 借助softmax回归，我们可以训练多分类的模型。
- 训练softmax回归循环模型与训练线性回归模型非常相似：先读取数据，再定义模型和损失函数，然后使用优化算法训练模型。大多数常见的深度学习模型都有类似的训练过程。

## 完整代码

```
from filecmp import cmp
from itertools import accumulate
from re import T
from tkinter import Y
import torch
import torchvision
import d2l


def softmax(X):
    # softmax function model
    X_exp = torch.exp(X)
    partition = X_exp.sum(1, keepdim=True)
    return X_exp/partition


num_of_inputs = 784  # pic dimension = 28x28 =784
num_of_outputs = 10  # output catagories are ten

# initialize parameters w,b (tensor)
W = torch.normal(0, 0.01, size=(num_of_inputs, num_of_outputs),
                 requires_grad=True)  # weight matrix is 784x10
b = torch.zeros(num_of_outputs, requires_grad=True)  # bias matrix is 10x1

# Initialize input matrix X, X's dimension is [2,5]
X = torch.normal(0, 1, (2, 5))
X_prob = softmax(X)
print(X_prob, X_prob.sum(1))


def net(X):
    return softmax(torch.matmul(X.reshape(-1, W.shape[0]), W)+b)


def cross_entropy(y_hat, y):
    # loss fuction y_hat is the predict value and the y is a label.
    return -torch.log(y_hat[range(len(y_hat)), Y])


def accuracy(y_hat, y):
    # calculate how many samples are predicted correctlly.
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:
        y_hat = y_hat.argmax(axis=1)
    cmp = y_hat.type(y.dtype) == y
    # cmp is a sign to compare the type of y_hat and y
    #  make sure y_hat has the same type with y
    return float(cmp.type(y.type).sum())
    # if these two types are different ,then we get 0


def evaluate_accuracy(net, data_iter):
    if isinstance(net, torch.nn.Module):
        net.eval()  # set the model into evaluation pattern, tell the model don't calculate the grad,only do the forward pass
    metric = Accumulateor(2)  # number of corret,number of predict
    with torch.no_grad():
        for X, y in data_iter:
            metric.add(accuracy(net(X), y), y.numel())
        return metric[0]/metric[1]


class Accumulateor:
    """在n个变量上累加"""

    def __init__(self, n):
        self.data = [0, 0]*n

    def add(self, *args):
        self.data = [a+float(b) for a, b in zip(self.data, args)]

    def reset(self):
        self.data = [0.0]*len(self.data)

    def __getittem__(self, idx):
        return self.data[idx]


def train_epcoch(net, train_iter, loss, updater):
    # 定义训练模型的一个迭代周期,其中updater是更新模型参数的常用函数
    if isinstance(net, torch.nn.Module):  # 把模型设置为训练模式
        net.train()
    metric = Accumulateor(3)  # 训练损失和，训练准确度的和，样本数
    for X, y in train_iter:  # 扫描一遍数据
        # 计算梯度并更新参数
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):  # 如果使用的是torch的内置优化函数
            updater.zero_grad()  # 先清除梯度
            l.mean().backward()  # 求出平均损失后反向传播求梯度
            updater.step()       # 更新所有参数信息
        else:
            # 使用定制的优化器和损失函数，已单独写出
            l.sum().backward()
            updater(X.shape[0])  # 这里的x.shape[0]实际上就是batch_size大小256
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    # 返回训练损失和训练精度
    return metric[0]/metric[2], metric[1]/metric[2]


class Animator:  # @save
    """在动画中绘制数据"""

    def __init__(self, xlabel=None, ylabel=None, legend=None, xlim=None,
                 ylim=None, xscale='linear', yscale='linear',
                 fmts=('-', 'm--', 'g-.', 'r:'), nrows=1, ncols=1,
                 figsize=(3.5, 2.5)):
        # 增量地绘制多条线
        if legend is None:
            legend = []
        d2l.use_svg_display()
        self.fig, self.axes = d2l.plt.subplots(nrows, ncols, figsize=figsize)
        if nrows * ncols == 1:
            self.axes = [self.axes, ]
        # 使用lambda函数捕获参数
        self.config_axes = lambda: d2l.set_axes(
            self.axes[0], xlabel, ylabel, xlim, ylim, xscale, yscale, legend)
        self.X, self.Y, self.fmts = None, None, fmts

    def add(self, x, y):
        # 向图表中添加多个数据点
        if not hasattr(y, "__len__"):
            y = [y]
        n = len(y)
        if not hasattr(x, "__len__"):
            x = [x] * n
        if not self.X:
            self.X = [[] for _ in range(n)]
        if not self.Y:
            self.Y = [[] for _ in range(n)]
        for i, (a, b) in enumerate(zip(x, y)):
            if a is not None and b is not None:
                self.X[i].append(a)
                self.Y[i].append(b)
        self.axes[0].cla()
        for x, y, fmt in zip(self.X, self.Y, self.fmts):
            self.axes[0].plot(x, y, fmt)
        self.config_axes()
        display.display(self.fig)
        display.clear_output(wait=True)


def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):  # @save
    """训练函数"""
    animator = Animator(xlabel='epoch', xlim=[1, num_epochs], ylim=[0.3, 0.9],
                        legend=['train loss', 'train acc', 'test acc'])
    # 可视化模块
    for epoch in range(num_epochs):  # 需要扫描N遍数据
        train_metrics = train_epcoch(net, train_iter, loss, updater)  # 进行一遍训练
        test_acc = evaluate_accuracy(net, test_iter)  # 测试集上跑精度
        animator.add(epoch + 1, train_metrics + (test_acc,))  # 在可视化模块上显示
    train_loss, train_acc = train_metrics
    assert train_loss < 0.5, train_loss
    assert train_acc <= 1 and train_acc > 0.7, train_acc
    assert test_acc <= 1 and test_acc > 0.7, test_acc


lr = 0.1


def updater(batch_size):
    return d2l.sgd([W, b], lr, batch_size)


num_epochs = 10
train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)


def predict_ch3(net, test_iter, n=6):  # @save
    """预测标签（定义见第3章）"""
    for X, y in test_iter:
        break
    trues = d2l.get_fashion_mnist_labels(y)
    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis=1))
    titles = [true + '\n' + pred for true, pred in zip(trues, preds)]
    d2l.show_images(
        X[0:n].reshape((n, 28, 28)), 1, n, titles=titles[0:n])


predict_ch3(net, test_iter)

```



# softmax的简洁实现

通过深度学习框架的高级API也能更方便地实现softmax回归模型。 本节如在 [3.6节](https://zh.d2l.ai/chapter_linear-networks/softmax-regression-scratch.html#sec-softmax-scratch)中一样， 继续使用Fashion-MNIST数据集，并保持批量大小为256。

```
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
```

## 初始化模型参数

softmax回归的输出层是一个全连接层。 因此，为了实现我们的模型， 我们只需在Sequential中添加一个带有10个输出的全连接层。 同样，在这里Sequential并不是必要的， 但它是实现深度模型的基础。 我们仍然以均值0和标准差0.01随机初始化权重。

```
# PyTorch不会隐式地调整输入的形状。因此，
# 我们在线性层前定义了展平层（flatten），来调整网络输入的形状
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std=0.01)

net.apply(init_weights);
```

我们计算了模型的输出，然后将此输出送入交叉熵损失。 从数学上讲，这是一件完全合理的事情。 然而，从计算角度来看，指数可能会造成数值稳定性问题。

![image-20220203172653131](image-20220203172653131-16438804151491.png)

```
loss = nn.CrossEntropyLoss(reduction='none')
```

## 优化算法

 在这里，我们使用学习率为0.1的小批量随机梯度下降作为优化算法。 这与我们在线性回归例子中的相同，这说明了优化器的普适性。

```
trainer = torch.optim.SGD(net.parameters(), lr=0.1)
```

## 训练

接下来我们调用 [3.6节](https://zh.d2l.ai/chapter_linear-networks/softmax-regression-scratch.html#sec-softmax-scratch)中 定义的训练函数来训练模型。

```
num_epochs = 10
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

![image-20220203174129799](image-20220203174129799-16438812923362.png)

和以前一样，这个算法使结果收敛到一个相当高的精度，而且这次的代码比之前更精简了。

## 小结

- 使用深度学习框架的高级API，我们可以更简洁地实现softmax回归。
- 从计算的角度来看，实现softmax回归比较复杂。在许多情况下，深度学习框架在这些著名的技巧之外采取了额外的预防措施，来确保数值的稳定性。这使我们避免了在实践中从零开始编写模型时可能遇到的陷阱。
